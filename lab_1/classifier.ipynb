{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d7a51d",
   "metadata": {},
   "source": [
    "#### Ввод числовых id двух людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_names = [476459668, 265329539]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ede27",
   "metadata": {},
   "source": [
    "#### Импорт библиотек и модулей парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fce122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lab1.parser_vk.lib.vk_friends_parser import VKClient, VKFriendsParser\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854321a2",
   "metadata": {},
   "source": [
    "#### Скачивание информации\n",
    "\n",
    "В этой ячейке в файл json скачивается информация о двух пользователях, их друзьях первого уровня (парсер для второго уровня в частности реализован в main.py).\n",
    "\n",
    "В people расположены признаки людей, а в edges id дружеских связей.\n",
    "\n",
    "Парсер импортирован из lab1.parser_vk.lib.vk_friends_parser\n",
    "\n",
    "Также тут реализовано создание файлов nodes_inference.json и edges_inference.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6177428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы инициализированы:\n",
      " - nodes_inference.json\n",
      " - edges_inference.csv\n",
      " - posts_count.csv\n",
      "=== Глубина 1/1, обрабатываем 1 пользователей ===\n",
      "Данные сохранены в user_476459668.json\n",
      "++++++++++++++++\n",
      "Данные пользователя 476459668 добавлены в общий список\n",
      "=== Глубина 1/1, обрабатываем 1 пользователей ===\n",
      "Данные сохранены в user_265329539.json\n",
      "++++++++++++++++\n",
      "Данные пользователя 265329539 добавлены в общий список\n",
      "Все данные объединены в nodes_inference.json\n"
     ]
    }
   ],
   "source": [
    "# edges = pd.read_csv('edges.csv')\n",
    "# sampled_edges = edges.drop_duplicates(subset='target_user_id')\n",
    "# sampled_edges = sampled_edges.sample(n=100)\n",
    "# set_ids = list(set(sampled_edges['target_user_id']) | set(sampled_edges['interactor_id']))\n",
    "\n",
    "\n",
    "# def nodes_maker(profile_names):\n",
    "\n",
    "\n",
    "def init_inference_files():\n",
    "    nodes_path = \"nodes_inference.json\"\n",
    "    with open(nodes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    edges_path = \"edges_inference.csv\"\n",
    "    with open(edges_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"target_user_id\", \"interactor_id\", \"likes_count\", \"comments_count\", \"reposts_count\"])\n",
    "    posts_path = \"posts_count.csv\"\n",
    "    with open(posts_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"posts\"])\n",
    "\n",
    "    print(\"Файлы инициализированы:\")\n",
    "    print(f\" - {nodes_path}\")\n",
    "    print(f\" - {edges_path}\")\n",
    "    print(f\" - {posts_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_inference_files()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VK_SERVICE_ACCESS_TOKEN = \"6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace\"\n",
    "# VK_SERVICE_ACCESS_TOKEN = os.getenv(\"VK_SERVICE_ACCESS_TOKEN\", None)\n",
    "# if VK_SERVICE_ACCESS_TOKEN is None:\n",
    "#     raise ValueError(\"VK_SERVICE_ACCESS_TOKEN не найден в переменных окружения\")\n",
    "\n",
    "vk = VKClient(token=VK_SERVICE_ACCESS_TOKEN)\n",
    "\n",
    "\n",
    "if os.path.exists(\"nodes_inference.json\"):\n",
    "    with open(\"nodes_inference.json\", 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            all_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            all_data = []\n",
    "else:\n",
    "    all_data = []\n",
    "\n",
    "for profile_name in profile_names:\n",
    "    root_id = profile_name\n",
    "\n",
    "    parser = VKFriendsParser(\n",
    "        vk_client=vk, save_photos=False\n",
    "    )\n",
    "    parser.fetch_network_fast([root_id], depth=1)\n",
    "\n",
    "    temp_filename = f\"user_{root_id}.json\"\n",
    "    parser.save_json(temp_filename)\n",
    "    print(f\"Данные сохранены в {temp_filename}\")\n",
    "    \n",
    "    with open(temp_filename, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            user_data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Ошибка чтения {temp_filename}: {e}\")\n",
    "            continue\n",
    "    print('++++++++++++++++')\n",
    "\n",
    "\n",
    "    if (user_data['people']):\n",
    "        # random_friend = random.choice(user_data['people'])\n",
    "        # random_friend_id = random.choice(user_data['people'])['id']\n",
    "        # print(random_friend)\n",
    "        # print(random_friend_id)\n",
    "        # parser.fetch_network_fast([random_friend_id], depth=1)\n",
    "\n",
    "        # # Сохраняем во временный файл\n",
    "        # temp_filename2 = f\"user_{random_friend_id}.json\"\n",
    "        # parser.save_json(temp_filename2)\n",
    "        # print(f\"Данные сохранены в {temp_filename2}\")\n",
    "        \n",
    "        # Читаем данные из временного файла\n",
    "        # with open(temp_filename2, 'r', encoding='utf-8') as f:\n",
    "        #     try:\n",
    "        #         user_data2 = json.load(f)\n",
    "        #     except json.JSONDecodeError as e:\n",
    "        #         print(f\"Ошибка чтения {temp_filename2}: {e}\")\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        if not isinstance(all_data, dict):\n",
    "            all_data = {\"people\": [], \"edges\": []}\n",
    "\n",
    "        if isinstance(user_data, dict) and \"people\" in user_data and \"edges\" in user_data:\n",
    "            all_data[\"people\"].extend(user_data[\"people\"])\n",
    "            all_data[\"edges\"].extend(user_data[\"edges\"])\n",
    "            # all_data[\"people\"].extend(user_data2[\"people\"])\n",
    "            # all_data[\"edges\"].extend(user_data2[\"edges\"])\n",
    "        else:\n",
    "            print(f\"Предупреждение: Некорректная структура данных для {profile_name}\")\n",
    "        \n",
    "        if os.path.exists(temp_filename):\n",
    "            os.remove(temp_filename)\n",
    "        # if os.path.exists(temp_filename2):\n",
    "        #     os.remove(temp_filename2)\n",
    "        print(f\"Данные пользователя {profile_name} добавлены в общий список\")\n",
    "\n",
    "with open(\"nodes_inference.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=5)\n",
    "\n",
    "print(\"Все данные объединены в nodes_inference.json\")\n",
    "\n",
    "\n",
    "# with open('nodes.json', 'r', encoding='utf-8') as f:\n",
    "#     json_file = json.load(f)\n",
    "\n",
    "# nodes_ids = pd.json_normalize(json_file['people'])['id'].unique()\n",
    "\n",
    "# for i in range(len(nodes_ids)):\n",
    "#     print(i)\n",
    "#     print(i/len(nodes_ids))\n",
    "#     node_id = nodes_ids[i]\n",
    "#     edges_parser(node_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30691f2",
   "metadata": {},
   "source": [
    "#### Скачивание в csv количества комментариев, лайков и репостов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500ac629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VKParser:\n",
    "    def __init__(self, access_token, version='5.131'):\n",
    "        self.access_token = access_token\n",
    "        self.version = version\n",
    "        self.base_url = 'https://api.vk.com/method/'\n",
    "        \n",
    "    def make_request(self, method, params):\n",
    "        \"\"\"Базовый метод для запросов к VK API\"\"\"\n",
    "        url = f\"{self.base_url}{method}\"\n",
    "        params.update({\n",
    "            'access_token': self.access_token,\n",
    "            'v': self.version\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Ошибка VK API: {data['error']['error_msg']}\")\n",
    "                return None\n",
    "                \n",
    "            return data['response']\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка запроса: {e}\")\n",
    "            return None\n",
    "\n",
    "    def resolve_screen_name(self, screen_name):\n",
    "        print(f\"Преобразуем короткое имя '{screen_name}' в ID...\")\n",
    "        \n",
    "        response = self.make_request('utils.resolveScreenName', {\n",
    "            'screen_name': screen_name\n",
    "        })\n",
    "        \n",
    "        if response and 'object_id' in response:\n",
    "            user_id = response['object_id']\n",
    "            print(f\"Найден ID: {user_id}\")\n",
    "            return user_id\n",
    "        else:\n",
    "            print(f\"Не удалось найти ID для '{screen_name}'\")\n",
    "            return None\n",
    "\n",
    "    def get_user_id(self, user_input):\n",
    "\n",
    "        if isinstance(user_input, int) or (isinstance(user_input, str) and user_input.isdigit()):\n",
    "            return int(user_input)\n",
    "        \n",
    "        # Если это короткое имя, преобразуем в ID\n",
    "        if isinstance(user_input, str) and not user_input.startswith('-'):\n",
    "            return self.resolve_screen_name(user_input)\n",
    "        \n",
    "        return user_input\n",
    "\n",
    "    def get_wall_posts(self, owner_id, count=100):\n",
    "        \"\"\"Получает посты со стены пользователя\"\"\"\n",
    "        print(f\"Получаем посты со стены пользователя {owner_id}...\")\n",
    "        \n",
    "        # Преобразуем owner_id в числовой формат если нужно\n",
    "        numeric_owner_id = self.get_user_id(owner_id)\n",
    "        if numeric_owner_id is None:\n",
    "            print(f\"Не удалось определить ID для {owner_id}\")\n",
    "            return []\n",
    "        \n",
    "        posts = []\n",
    "        offset = 0\n",
    "        max_posts = count\n",
    "        \n",
    "        while len(posts) < max_posts:\n",
    "            response = self.make_request('wall.get', {\n",
    "                'owner_id': numeric_owner_id,\n",
    "                'count': min(100, max_posts - len(posts)),\n",
    "                'offset': offset,\n",
    "                'extended': 1\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            posts.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) == 0:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        print(f\"Получено {len(posts)} постов\")\n",
    "\n",
    "        posts_count = pd.read_csv('posts_count.csv')\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'id': [numeric_owner_id],\n",
    "            'posts': [len(posts)]\n",
    "        })\n",
    "\n",
    "        posts_count = pd.concat([posts_count, new_row], ignore_index=True)\n",
    "        posts_count.to_csv('posts_count.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "        return posts, numeric_owner_id\n",
    "    \n",
    "    def get_likes(self, owner_id, item_id, item_type='post'):\n",
    "        \"\"\"Получает список пользователей, лайкнувших запись\"\"\"\n",
    "        likes = []\n",
    "        offset = 0\n",
    "        count = 1000\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('likes.getList', {\n",
    "                'type': item_type,\n",
    "                'owner_id': owner_id,\n",
    "                'item_id': item_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'filter': 'likes'\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            likes.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return likes\n",
    "    \n",
    "    def get_comments(self, owner_id, post_id):\n",
    "        \"\"\"Получает комментарии к посту\"\"\"\n",
    "        comments = []\n",
    "        offset = 0\n",
    "        count = 100\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('wall.getComments', {\n",
    "                'owner_id': owner_id,\n",
    "                'post_id': post_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'extended': 0\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            for comment in response['items']:\n",
    "                comment_data = {\n",
    "                    'id': comment['id'],\n",
    "                    'from_id': comment['from_id'],\n",
    "                    'date': comment['date'],\n",
    "                    'text': comment['text'],\n",
    "                    'likes': comment.get('likes', {}).get('count', 0)\n",
    "                }\n",
    "                comments.append(comment_data)\n",
    "            \n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def get_reposts(self, owner_id, post_id):\n",
    "        \"\"\"Получает информацию о репостах\"\"\"\n",
    "        response = self.make_request('wall.getReposts', {\n",
    "            'owner_id': owner_id,\n",
    "            'post_id': post_id,\n",
    "            'count': 1000\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return []\n",
    "            \n",
    "        reposts = []\n",
    "        if 'items' in response:\n",
    "            for repost in response['items']:\n",
    "                repost_data = {\n",
    "                    'id': repost['id'],\n",
    "                    'from_id': repost['from_id'],\n",
    "                    'date': repost['date'],\n",
    "                    'text': repost.get('text', ''),\n",
    "                    'copy_history': repost.get('copy_history', [])\n",
    "                }\n",
    "                reposts.append(repost_data)\n",
    "        \n",
    "        return reposts\n",
    "    \n",
    "    def get_user_info(self, user_ids):\n",
    "        \"\"\"Получает информацию о пользователях\"\"\"\n",
    "        if not user_ids:\n",
    "            return {}\n",
    "            \n",
    "        numeric_ids = []\n",
    "        for user_id in user_ids:\n",
    "            if isinstance(user_id, int) or (isinstance(user_id, str) and user_id.lstrip('-').isdigit()):\n",
    "                numeric_ids.append(str(user_id))\n",
    "        \n",
    "        if not numeric_ids:\n",
    "            return {}\n",
    "            \n",
    "        response = self.make_request('users.get', {\n",
    "            'user_ids': ','.join(numeric_ids),\n",
    "            'fields': 'first_name,last_name,sex,bdate,city,country,screen_name'\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return {}\n",
    "            \n",
    "        user_info = {}\n",
    "        for user in response:\n",
    "            user_info[user['id']] = {\n",
    "                'first_name': user.get('first_name', ''),\n",
    "                'last_name': user.get('last_name', ''),\n",
    "                'sex': user.get('sex', 0),\n",
    "                'bdate': user.get('bdate', ''),\n",
    "                'city': user.get('city', {}).get('title', '') if 'city' in user else '',\n",
    "                'country': user.get('country', {}).get('title', '') if 'country' in user else '',\n",
    "                'screen_name': user.get('screen_name', '')\n",
    "            }\n",
    "        \n",
    "        return user_info\n",
    "    \n",
    "    def process_post(self, post, owner_id):\n",
    "        \"\"\"Обрабатывает один пост и собирает всю информацию\"\"\"\n",
    "        post_id = post['id']\n",
    "        \n",
    "        print(f\"Обрабатываем пост {post_id}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_id': post_id,\n",
    "            'owner_id': owner_id,\n",
    "            'date': post['date'],\n",
    "            'text': post.get('text', '')[:500],\n",
    "            'likes_count': post.get('likes', {}).get('count', 0),\n",
    "            'comments_count': post.get('comments', {}).get('count', 0),\n",
    "            'reposts_count': post.get('reposts', {}).get('count', 0),\n",
    "            'views_count': post.get('views', {}).get('count', 0) if 'views' in post else 0\n",
    "        }\n",
    "        \n",
    "        if 'copy_history' in post and post['copy_history']:\n",
    "            original_post = post['copy_history'][0]\n",
    "            post_data['is_repost'] = True\n",
    "            post_data['reposted_from'] = {\n",
    "                'owner_id': original_post['owner_id'],\n",
    "                'post_id': original_post['id'],\n",
    "                'text': original_post.get('text', '')[:500]\n",
    "            }\n",
    "        else:\n",
    "            post_data['is_repost'] = False\n",
    "            post_data['reposted_from'] = None\n",
    "        \n",
    "        if isinstance(owner_id, int):\n",
    "            print(f\"  Собираем лайки...\")\n",
    "            post_data['likes'] = self.get_likes(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем комментарии...\")\n",
    "            post_data['comments'] = self.get_comments(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем репосты...\")\n",
    "            post_data['reposts'] = self.get_reposts(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "        else:\n",
    "            print(f\"  Пропускаем сбор лайков/комментариев - неверный owner_id\")\n",
    "            post_data['likes'] = []\n",
    "            post_data['comments'] = []\n",
    "            post_data['reposts'] = []\n",
    "        \n",
    "        return post_data\n",
    "    \n",
    "    def parse_user_wall(self, user_input, max_posts=50):\n",
    "        print(f\"\\n=== Начинаем парсинг стены пользователя {user_input} ===\")\n",
    "        \n",
    "        posts, numeric_owner_id = self.get_wall_posts(user_input, max_posts)\n",
    "        processed_posts = []\n",
    "        \n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"Пост {i+1}/{len(posts)}\")\n",
    "            processed_post = self.process_post(post, numeric_owner_id)\n",
    "            processed_posts.append(processed_post)\n",
    "            \n",
    "            if i < len(posts) - 1:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return processed_posts, numeric_owner_id\n",
    "    \n",
    "    def collect_all_user_ids(self, posts_data):\n",
    "        user_ids = set()\n",
    "        \n",
    "        for post in posts_data:\n",
    "            user_ids.update(post['likes'])\n",
    "            \n",
    "            for comment in post['comments']:\n",
    "                user_ids.add(comment['from_id'])\n",
    "            \n",
    "            for repost in post['reposts']:\n",
    "                user_ids.add(repost['from_id'])\n",
    "        \n",
    "        return list(user_ids)\n",
    "    \n",
    "    def analyze_two_users(self, user1_input, user2_input, max_posts_per_user=20):\n",
    "        print(f\"Запускаем анализ пользователей {user1_input} и {user2_input}\")\n",
    "        \n",
    "        user1_posts, user1_id = self.parse_user_wall(user1_input, max_posts_per_user)\n",
    "        user2_posts, user2_id = self.parse_user_wall(user2_input, max_posts_per_user)\n",
    "        \n",
    "        all_user_ids = set()\n",
    "        all_user_ids.update(self.collect_all_user_ids(user1_posts))\n",
    "        all_user_ids.update(self.collect_all_user_ids(user2_posts))\n",
    "        \n",
    "        print(f\"Собираем информацию о {len(all_user_ids)} пользователях...\")\n",
    "        user_info = self.get_user_info(list(all_user_ids))\n",
    "        \n",
    "        result = {\n",
    "            'analysis_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'user1_input': user1_input,\n",
    "                'user2_input': user2_input,\n",
    "                'user1_id': user1_id,\n",
    "                'user2_id': user2_id,\n",
    "                'total_posts_analyzed': len(user1_posts) + len(user2_posts),\n",
    "                'total_users_found': len(all_user_ids)\n",
    "            },\n",
    "            'user1_posts': user1_posts,\n",
    "            'user2_posts': user2_posts,\n",
    "            'user_info': user_info\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_to_json(self, data, filename=None):\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user1 = str(data['analysis_info']['user1_input']).replace('/', '_')\n",
    "            user2 = str(data['analysis_info']['user2_input']).replace('/', '_')\n",
    "            filename = f\"vk_analysis_{user1}_{user2}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Данные сохранены в файл: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def generate_interaction_csv(self, user_input, output_filename=None, max_posts=100):\n",
    "        print(f\"Генерируем CSV со статистикой взаимодействий для {user_input}...\")\n",
    "        \n",
    "\n",
    "        posts, owner_id = self.parse_user_wall(user_input, max_posts)\n",
    "        \n",
    "        if not posts:\n",
    "            print(\"Не удалось получить посты пользователя\")\n",
    "            return None\n",
    "        \n",
    "        interactions = defaultdict(lambda: {'likes': 0, 'comments': 0, 'reposts': 0})\n",
    "        \n",
    "        for post in posts:\n",
    "            self._process_post_for_interactions(post, interactions, owner_id)\n",
    "        \n",
    "        if not output_filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user_identifier = str(user_input).replace('/', '_')\n",
    "            output_filename = f\"vk_interactions_{user_identifier}_{timestamp}.csv\"\n",
    "        \n",
    "        return self._write_interactions_to_csv(owner_id, interactions, output_filename)\n",
    "    \n",
    "    def _process_post_for_interactions(self, post, interactions, owner_id):\n",
    "        post_id = post['post_id']\n",
    "        \n",
    "        for like_user_id in post.get('likes', []):\n",
    "            interactions[like_user_id]['likes'] += 1\n",
    "        \n",
    "        for comment in post.get('comments', []):\n",
    "            commenter_id = comment['from_id']\n",
    "            interactions[commenter_id]['comments'] += 1\n",
    "        \n",
    "        for repost in post.get('reposts', []):\n",
    "            reposter_id = repost['from_id']\n",
    "            interactions[reposter_id]['reposts'] += 1\n",
    "    \n",
    "    def _write_interactions_to_csv(self, owner_id, interactions, output_filename):\n",
    "        target_file = 'edges_inference.csv'\n",
    "\n",
    "        try:\n",
    "            edges = pd.read_csv(target_file)\n",
    "        except FileNotFoundError:\n",
    "            edges = pd.DataFrame(columns=[\n",
    "                'target_user_id',\n",
    "                'interactor_id',\n",
    "                'likes_count',\n",
    "                'comments_count',\n",
    "                'reposts_count'\n",
    "            ])\n",
    "\n",
    "\n",
    "        new_rows = []\n",
    "        for interactor_id, stats in interactions.items():\n",
    "            new_rows.append({\n",
    "                'target_user_id': owner_id,\n",
    "                'interactor_id': interactor_id,\n",
    "                'likes_count': stats['likes'],\n",
    "                'comments_count': stats['comments'],\n",
    "                'reposts_count': stats['reposts']\n",
    "            })\n",
    "\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            edges = pd.concat([edges, new_df], ignore_index=True)\n",
    "\n",
    "        print(edges)\n",
    "        edges.to_csv(target_file, index=False)\n",
    "\n",
    "        print(f\"CSV файл обновлён: {target_file}\")\n",
    "        print(f\"Добавлено взаимодействий: {len(new_rows)}\")\n",
    "        return target_file\n",
    "\n",
    "def edges_parser(wall_id):\n",
    "    # Настройки\n",
    "    VK_ACCESS_TOKEN = '6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace'\n",
    "    # Создаем парсер\n",
    "    parser = VKParser(VK_ACCESS_TOKEN)\n",
    "\n",
    "    csv_file = parser.generate_interaction_csv(wall_id, max_posts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6421be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерируем CSV со статистикой взаимодействий для 476459668...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 476459668 ===\n",
      "Получаем посты со стены пользователя 476459668...\n",
      "Получено 6 постов\n",
      "Пост 1/6\n",
      "Обрабатываем пост 11...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Пост 2/6\n",
      "Обрабатываем пост 10...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Пост 3/6\n",
      "Обрабатываем пост 9...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Пост 4/6\n",
      "Обрабатываем пост 8...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Пост 5/6\n",
      "Обрабатываем пост 6...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Пост 6/6\n",
      "Обрабатываем пост 5...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "   target_user_id interactor_id likes_count comments_count reposts_count\n",
      "0       476459668     265329539           2              1             0\n",
      "1       476459668     432213567           1              0             0\n",
      "2       476459668      91981299           1              0             0\n",
      "3       476459668     189446391           1              0             0\n",
      "4       476459668     218313726           1              0             0\n",
      "5       476459668     223074199           1              0             0\n",
      "6       476459668     241501346           1              0             0\n",
      "7       476459668     263746090           1              0             0\n",
      "8       476459668     398917086           1              0             0\n",
      "9       476459668     706838635           1              0             0\n",
      "10      476459668     740335509           1              0             0\n",
      "11      476459668     526035141           0              1             0\n",
      "CSV файл обновлён: edges_inference.csv\n",
      "Добавлено взаимодействий: 12\n",
      "Генерируем CSV со статистикой взаимодействий для 265329539...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 265329539 ===\n",
      "Получаем посты со стены пользователя 265329539...\n",
      "Получено 0 постов\n",
      "Не удалось получить посты пользователя\n"
     ]
    }
   ],
   "source": [
    "for i in profile_names:\n",
    "    edges_parser(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a3c89",
   "metadata": {},
   "source": [
    "#### Загрузка json и csv файлов в дата фреймы признаков вершин и рёбер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222ea7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_inference.json', 'r', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "    nodes_inference = pd.json_normalize(json_file['people'])\n",
    "\n",
    "edges_inference = pd.read_csv('edges_inference.csv')\n",
    "# sampled_edges_inference = edges_inference.drop_duplicates(subset='target_user_id')\n",
    "# set_ids2 = list(set(sampled_edges_inference['target_user_id']) | set(sampled_edges_inference['interactor_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab082a",
   "metadata": {},
   "source": [
    "#### Определение наличия дружбы между людьми и добавление в дата фрейм рёбер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed01acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship = pd.DataFrame(json_file['edges']).drop(columns=[2]).rename(columns={0: 'user_id', 1: 'friend_id'})\n",
    "\n",
    "friendship_set = set(friendship.apply(lambda row: frozenset([row['user_id'], row['friend_id']]), axis=1))\n",
    "\n",
    "\n",
    "def is_friend_pair(row):\n",
    "    pair = frozenset([row['target_user_id'], row['interactor_id']])\n",
    "    return pair in friendship_set\n",
    "\n",
    "\n",
    "edges_inference['is_friend'] = edges_inference.apply(is_friend_pair, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19b661",
   "metadata": {},
   "source": [
    "#### Агрегация информации у целевых пользователей и обработка в случае отсутствия связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf5fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target_user_id  interactor_id  likes_count  comments_count  reposts_count  \\\n",
      "0       476459668      265329539            2               1              0   \n",
      "\n",
      "   common_friends_count  is_friend  \n",
      "0                     7          1  \n"
     ]
    }
   ],
   "source": [
    "friends_dict = defaultdict(set)\n",
    "for _, row in friendship.iterrows():\n",
    "    friends_dict[row['user_id']].add(row['friend_id'])\n",
    "    friends_dict[row['friend_id']].add(row['user_id'])\n",
    "\n",
    "\n",
    "def count_common_friends(u1, u2):\n",
    "    return len(friends_dict[u1] & friends_dict[u2])\n",
    "\n",
    "\n",
    "def check_is_friend(u1, u2):\n",
    "    return int(u2 in friends_dict[u1])\n",
    "\n",
    "\n",
    "edges_inference['common_friends_count'] = edges_inference.apply(\n",
    "    lambda row: count_common_friends(row['target_user_id'], row['interactor_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "mask = (\n",
    "    ((edges_inference['target_user_id'] == profile_names[0]) & (edges_inference['interactor_id'] == profile_names[1])) |\n",
    "    ((edges_inference['target_user_id'] == profile_names[1]) & (edges_inference['interactor_id'] == profile_names[0]))\n",
    ")\n",
    "\n",
    "pair_edges = edges_inference[mask]\n",
    "\n",
    "\n",
    "common_friends = count_common_friends(profile_names[0], profile_names[1])\n",
    "is_friend = check_is_friend(profile_names[0], profile_names[1])\n",
    "\n",
    "\n",
    "if pair_edges.empty:\n",
    "    agg_row = {\n",
    "        'target_user_id': profile_names[0],\n",
    "        'interactor_id': profile_names[1],\n",
    "        'likes_count': 0,\n",
    "        'comments_count': 0,\n",
    "        'reposts_count': 0,\n",
    "        'common_friends_count': common_friends,\n",
    "        'is_friend': is_friend\n",
    "    }\n",
    "else:\n",
    "    agg_row = {\n",
    "        'target_user_id': profile_names[0],\n",
    "        'interactor_id': profile_names[1],\n",
    "        'likes_count': pair_edges['likes_count'].sum(),\n",
    "        'comments_count': pair_edges['comments_count'].sum(),\n",
    "        'reposts_count': pair_edges['reposts_count'].sum(),\n",
    "        'common_friends_count': common_friends,\n",
    "        'is_friend': is_friend\n",
    "    }\n",
    "\n",
    "result_df = pd.DataFrame([agg_row])\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce51ce6",
   "metadata": {},
   "source": [
    "#### Загрузка предобученной модели и получение предсказаний\n",
    "\n",
    "В данном случае демонстрируется классификация дружбы двух пользователей, id которых заданы в начале. По сумме их совместных лайков, комментариев и общих друзей происходит бинарная классификация. В начале выводится confusion_matrix, потом её интерпретация, а затем вероятность предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80be7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "feature_columns = ['likes_count', 'comments_count', 'common_friends_count']\n",
    "\n",
    "\n",
    "\n",
    "X = result_df[feature_columns].fillna(0)\n",
    "y = result_df['is_friend'].astype(int)\n",
    "\n",
    "\n",
    "y_pred = loaded_model.predict(X)\n",
    "y_pred_proba = loaded_model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f59262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "[[0 0]\n",
      " [1 0]]\n",
      "Ошибка: предсказано — не друзья, на самом деле — друзья.\n",
      "[0.23972377]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "def prediction_result(y_true, y_pred):\n",
    "    if y_true[0] == y_pred[0]:\n",
    "        if y_true[0] == 1:\n",
    "            print(\"Верное предсказание: они являются друзьями.\")\n",
    "        else:\n",
    "            print(\"Верное предсказание: они не являются друзьями.\")\n",
    "    else:\n",
    "        if y_true[0] == 1:\n",
    "            print(\"Ошибка: предсказано — не друзья, на самом деле — друзья.\")\n",
    "        else:\n",
    "            print(\"Ошибка: предсказано — друзья, на самом деле — не друзья.\")\n",
    "\n",
    "prediction_result(y, y_pred)\n",
    "\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2217bc",
   "metadata": {},
   "source": [
    "#### Демонстрация классификации на расширенной тестовой выборке\n",
    "\n",
    "Импортируется датафрейм сбалансированных тестовых данных на 1403 записи для демонстрации процесса классификации и метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b67548ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>comments_count</th>\n",
       "      <th>common_friends_count</th>\n",
       "      <th>is_friend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4568</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5468</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4409</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6791</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>3770</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>6251</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>2374</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>6719</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1403 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  likes_count  comments_count  common_friends_count  is_friend\n",
       "0            467            2               0                     3          0\n",
       "1           4568            4               0                     1          0\n",
       "2           5468            3               0                     1          0\n",
       "3           4409            2               0                     0          1\n",
       "4           6791            2               1                     0          1\n",
       "...          ...          ...             ...                   ...        ...\n",
       "1398        3770            0               1                     3          0\n",
       "1399        6251            4               1                     0          1\n",
       "1400        2374            5               1                     0          1\n",
       "1401        6719            1               0                     1          0\n",
       "1402         599            1               0                     0          0\n",
       "\n",
       "[1403 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('model_data.csv')\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6565d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       871\n",
      "           1       0.66      0.93      0.77       532\n",
      "\n",
      "    accuracy                           0.79      1403\n",
      "   macro avg       0.80      0.82      0.79      1403\n",
      "weighted avg       0.84      0.79      0.80      1403\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[615 256]\n",
      " [ 35 497]]\n"
     ]
    }
   ],
   "source": [
    "feature_columns = ['likes_count', 'comments_count', 'common_friends_count']\n",
    "\n",
    "X_test = model_data[feature_columns].fillna(0)\n",
    "y_test = model_data['is_friend'].astype(int)\n",
    "\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_pred_proba = loaded_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
