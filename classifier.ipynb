{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53e720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_names = [432213567, 242504512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51fce122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lab1.parser_vk.lib.vk_friends_parser import VKClient, VKFriendsParser\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6177428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Глубина 1/1, обрабатываем 1 пользователей ===\n",
      "Данные сохранены в user_432213567.json\n",
      "++++++++++++++++\n",
      "Данные пользователя 432213567 добавлены в общий список\n",
      "=== Глубина 1/1, обрабатываем 1 пользователей ===\n",
      "Данные сохранены в user_242504512.json\n",
      "++++++++++++++++\n",
      "Данные пользователя 242504512 добавлены в общий список\n",
      "Все данные объединены в nodes_inference.json\n"
     ]
    }
   ],
   "source": [
    "# edges = pd.read_csv('edges.csv')\n",
    "# sampled_edges = edges.drop_duplicates(subset='target_user_id')\n",
    "# sampled_edges = sampled_edges.sample(n=100)\n",
    "# set_ids = list(set(sampled_edges['target_user_id']) | set(sampled_edges['interactor_id']))\n",
    "\n",
    "\n",
    "# def nodes_maker(profile_names):\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VK_SERVICE_ACCESS_TOKEN = \"6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace\"\n",
    "# VK_SERVICE_ACCESS_TOKEN = os.getenv(\"VK_SERVICE_ACCESS_TOKEN\", None)\n",
    "# if VK_SERVICE_ACCESS_TOKEN is None:\n",
    "#     raise ValueError(\"VK_SERVICE_ACCESS_TOKEN не найден в переменных окружения\")\n",
    "\n",
    "vk = VKClient(token=VK_SERVICE_ACCESS_TOKEN)\n",
    "\n",
    "\n",
    "if os.path.exists(\"nodes_inference.json\"):\n",
    "    with open(\"nodes_inference.json\", 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            all_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            all_data = []\n",
    "else:\n",
    "    all_data = []\n",
    "\n",
    "for profile_name in profile_names:\n",
    "    root_id = profile_name\n",
    "\n",
    "    parser = VKFriendsParser(\n",
    "        vk_client=vk, save_photos=False\n",
    "    )\n",
    "    parser.fetch_network_fast([root_id], depth=1)\n",
    "\n",
    "    temp_filename = f\"user_{root_id}.json\"\n",
    "    parser.save_json(temp_filename)\n",
    "    print(f\"Данные сохранены в {temp_filename}\")\n",
    "    \n",
    "    with open(temp_filename, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            user_data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Ошибка чтения {temp_filename}: {e}\")\n",
    "            continue\n",
    "    print('++++++++++++++++')\n",
    "\n",
    "\n",
    "    if (user_data['people']):\n",
    "        # random_friend = random.choice(user_data['people'])\n",
    "        # random_friend_id = random.choice(user_data['people'])['id']\n",
    "        # print(random_friend)\n",
    "        # print(random_friend_id)\n",
    "        # parser.fetch_network_fast([random_friend_id], depth=1)\n",
    "\n",
    "        # # Сохраняем во временный файл\n",
    "        # temp_filename2 = f\"user_{random_friend_id}.json\"\n",
    "        # parser.save_json(temp_filename2)\n",
    "        # print(f\"Данные сохранены в {temp_filename2}\")\n",
    "        \n",
    "        # Читаем данные из временного файла\n",
    "        # with open(temp_filename2, 'r', encoding='utf-8') as f:\n",
    "        #     try:\n",
    "        #         user_data2 = json.load(f)\n",
    "        #     except json.JSONDecodeError as e:\n",
    "        #         print(f\"Ошибка чтения {temp_filename2}: {e}\")\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        if not isinstance(all_data, dict):\n",
    "            all_data = {\"people\": [], \"edges\": []}\n",
    "\n",
    "        if isinstance(user_data, dict) and \"people\" in user_data and \"edges\" in user_data:\n",
    "            all_data[\"people\"].extend(user_data[\"people\"])\n",
    "            all_data[\"edges\"].extend(user_data[\"edges\"])\n",
    "            # all_data[\"people\"].extend(user_data2[\"people\"])\n",
    "            # all_data[\"edges\"].extend(user_data2[\"edges\"])\n",
    "        else:\n",
    "            print(f\"Предупреждение: Некорректная структура данных для {profile_name}\")\n",
    "        \n",
    "        if os.path.exists(temp_filename):\n",
    "            os.remove(temp_filename)\n",
    "        # if os.path.exists(temp_filename2):\n",
    "        #     os.remove(temp_filename2)\n",
    "        print(f\"Данные пользователя {profile_name} добавлены в общий список\")\n",
    "\n",
    "with open(\"nodes_inference.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=5)\n",
    "\n",
    "print(\"Все данные объединены в nodes_inference.json\")\n",
    "\n",
    "\n",
    "# with open('nodes.json', 'r', encoding='utf-8') as f:\n",
    "#     json_file = json.load(f)\n",
    "\n",
    "# nodes_ids = pd.json_normalize(json_file['people'])['id'].unique()\n",
    "\n",
    "# for i in range(len(nodes_ids)):\n",
    "#     print(i)\n",
    "#     print(i/len(nodes_ids))\n",
    "#     node_id = nodes_ids[i]\n",
    "#     edges_parser(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ac629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VKParser:\n",
    "    def __init__(self, access_token, version='5.131'):\n",
    "        self.access_token = access_token\n",
    "        self.version = version\n",
    "        self.base_url = 'https://api.vk.com/method/'\n",
    "        \n",
    "    def make_request(self, method, params):\n",
    "        \"\"\"Базовый метод для запросов к VK API\"\"\"\n",
    "        url = f\"{self.base_url}{method}\"\n",
    "        params.update({\n",
    "            'access_token': self.access_token,\n",
    "            'v': self.version\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Ошибка VK API: {data['error']['error_msg']}\")\n",
    "                return None\n",
    "                \n",
    "            return data['response']\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка запроса: {e}\")\n",
    "            return None\n",
    "\n",
    "    def resolve_screen_name(self, screen_name):\n",
    "        print(f\"Преобразуем короткое имя '{screen_name}' в ID...\")\n",
    "        \n",
    "        response = self.make_request('utils.resolveScreenName', {\n",
    "            'screen_name': screen_name\n",
    "        })\n",
    "        \n",
    "        if response and 'object_id' in response:\n",
    "            user_id = response['object_id']\n",
    "            print(f\"Найден ID: {user_id}\")\n",
    "            return user_id\n",
    "        else:\n",
    "            print(f\"Не удалось найти ID для '{screen_name}'\")\n",
    "            return None\n",
    "\n",
    "    def get_user_id(self, user_input):\n",
    "\n",
    "        if isinstance(user_input, int) or (isinstance(user_input, str) and user_input.isdigit()):\n",
    "            return int(user_input)\n",
    "        \n",
    "        # Если это короткое имя, преобразуем в ID\n",
    "        if isinstance(user_input, str) and not user_input.startswith('-'):\n",
    "            return self.resolve_screen_name(user_input)\n",
    "        \n",
    "        return user_input\n",
    "\n",
    "    def get_wall_posts(self, owner_id, count=100):\n",
    "        \"\"\"Получает посты со стены пользователя\"\"\"\n",
    "        print(f\"Получаем посты со стены пользователя {owner_id}...\")\n",
    "        \n",
    "        # Преобразуем owner_id в числовой формат если нужно\n",
    "        numeric_owner_id = self.get_user_id(owner_id)\n",
    "        if numeric_owner_id is None:\n",
    "            print(f\"Не удалось определить ID для {owner_id}\")\n",
    "            return []\n",
    "        \n",
    "        posts = []\n",
    "        offset = 0\n",
    "        max_posts = count\n",
    "        \n",
    "        while len(posts) < max_posts:\n",
    "            response = self.make_request('wall.get', {\n",
    "                'owner_id': numeric_owner_id,\n",
    "                'count': min(100, max_posts - len(posts)),\n",
    "                'offset': offset,\n",
    "                'extended': 1\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            posts.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) == 0:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        print(f\"Получено {len(posts)} постов\")\n",
    "\n",
    "        posts_count = pd.read_csv('posts_count.csv')\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'id': [numeric_owner_id],\n",
    "            'posts': [len(posts)]\n",
    "        })\n",
    "\n",
    "        posts_count = pd.concat([posts_count, new_row], ignore_index=True)\n",
    "        posts_count.to_csv('posts_count.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "        return posts, numeric_owner_id\n",
    "    \n",
    "    def get_likes(self, owner_id, item_id, item_type='post'):\n",
    "        \"\"\"Получает список пользователей, лайкнувших запись\"\"\"\n",
    "        likes = []\n",
    "        offset = 0\n",
    "        count = 1000\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('likes.getList', {\n",
    "                'type': item_type,\n",
    "                'owner_id': owner_id,\n",
    "                'item_id': item_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'filter': 'likes'\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            likes.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return likes\n",
    "    \n",
    "    def get_comments(self, owner_id, post_id):\n",
    "        \"\"\"Получает комментарии к посту\"\"\"\n",
    "        comments = []\n",
    "        offset = 0\n",
    "        count = 100\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('wall.getComments', {\n",
    "                'owner_id': owner_id,\n",
    "                'post_id': post_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'extended': 0\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            for comment in response['items']:\n",
    "                comment_data = {\n",
    "                    'id': comment['id'],\n",
    "                    'from_id': comment['from_id'],\n",
    "                    'date': comment['date'],\n",
    "                    'text': comment['text'],\n",
    "                    'likes': comment.get('likes', {}).get('count', 0)\n",
    "                }\n",
    "                comments.append(comment_data)\n",
    "            \n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def get_reposts(self, owner_id, post_id):\n",
    "        \"\"\"Получает информацию о репостах\"\"\"\n",
    "        response = self.make_request('wall.getReposts', {\n",
    "            'owner_id': owner_id,\n",
    "            'post_id': post_id,\n",
    "            'count': 1000\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return []\n",
    "            \n",
    "        reposts = []\n",
    "        if 'items' in response:\n",
    "            for repost in response['items']:\n",
    "                repost_data = {\n",
    "                    'id': repost['id'],\n",
    "                    'from_id': repost['from_id'],\n",
    "                    'date': repost['date'],\n",
    "                    'text': repost.get('text', ''),\n",
    "                    'copy_history': repost.get('copy_history', [])\n",
    "                }\n",
    "                reposts.append(repost_data)\n",
    "        \n",
    "        return reposts\n",
    "    \n",
    "    def get_user_info(self, user_ids):\n",
    "        \"\"\"Получает информацию о пользователях\"\"\"\n",
    "        if not user_ids:\n",
    "            return {}\n",
    "            \n",
    "        numeric_ids = []\n",
    "        for user_id in user_ids:\n",
    "            if isinstance(user_id, int) or (isinstance(user_id, str) and user_id.lstrip('-').isdigit()):\n",
    "                numeric_ids.append(str(user_id))\n",
    "        \n",
    "        if not numeric_ids:\n",
    "            return {}\n",
    "            \n",
    "        response = self.make_request('users.get', {\n",
    "            'user_ids': ','.join(numeric_ids),\n",
    "            'fields': 'first_name,last_name,sex,bdate,city,country,screen_name'\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return {}\n",
    "            \n",
    "        user_info = {}\n",
    "        for user in response:\n",
    "            user_info[user['id']] = {\n",
    "                'first_name': user.get('first_name', ''),\n",
    "                'last_name': user.get('last_name', ''),\n",
    "                'sex': user.get('sex', 0),\n",
    "                'bdate': user.get('bdate', ''),\n",
    "                'city': user.get('city', {}).get('title', '') if 'city' in user else '',\n",
    "                'country': user.get('country', {}).get('title', '') if 'country' in user else '',\n",
    "                'screen_name': user.get('screen_name', '')\n",
    "            }\n",
    "        \n",
    "        return user_info\n",
    "    \n",
    "    def process_post(self, post, owner_id):\n",
    "        \"\"\"Обрабатывает один пост и собирает всю информацию\"\"\"\n",
    "        post_id = post['id']\n",
    "        \n",
    "        print(f\"Обрабатываем пост {post_id}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_id': post_id,\n",
    "            'owner_id': owner_id,\n",
    "            'date': post['date'],\n",
    "            'text': post.get('text', '')[:500],\n",
    "            'likes_count': post.get('likes', {}).get('count', 0),\n",
    "            'comments_count': post.get('comments', {}).get('count', 0),\n",
    "            'reposts_count': post.get('reposts', {}).get('count', 0),\n",
    "            'views_count': post.get('views', {}).get('count', 0) if 'views' in post else 0\n",
    "        }\n",
    "        \n",
    "        if 'copy_history' in post and post['copy_history']:\n",
    "            original_post = post['copy_history'][0]\n",
    "            post_data['is_repost'] = True\n",
    "            post_data['reposted_from'] = {\n",
    "                'owner_id': original_post['owner_id'],\n",
    "                'post_id': original_post['id'],\n",
    "                'text': original_post.get('text', '')[:500]\n",
    "            }\n",
    "        else:\n",
    "            post_data['is_repost'] = False\n",
    "            post_data['reposted_from'] = None\n",
    "        \n",
    "        if isinstance(owner_id, int):\n",
    "            print(f\"  Собираем лайки...\")\n",
    "            post_data['likes'] = self.get_likes(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем комментарии...\")\n",
    "            post_data['comments'] = self.get_comments(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем репосты...\")\n",
    "            post_data['reposts'] = self.get_reposts(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "        else:\n",
    "            print(f\"  Пропускаем сбор лайков/комментариев - неверный owner_id\")\n",
    "            post_data['likes'] = []\n",
    "            post_data['comments'] = []\n",
    "            post_data['reposts'] = []\n",
    "        \n",
    "        return post_data\n",
    "    \n",
    "    def parse_user_wall(self, user_input, max_posts=50):\n",
    "        print(f\"\\n=== Начинаем парсинг стены пользователя {user_input} ===\")\n",
    "        \n",
    "        posts, numeric_owner_id = self.get_wall_posts(user_input, max_posts)\n",
    "        processed_posts = []\n",
    "        \n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"Пост {i+1}/{len(posts)}\")\n",
    "            processed_post = self.process_post(post, numeric_owner_id)\n",
    "            processed_posts.append(processed_post)\n",
    "            \n",
    "            if i < len(posts) - 1:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return processed_posts, numeric_owner_id\n",
    "    \n",
    "    def collect_all_user_ids(self, posts_data):\n",
    "        user_ids = set()\n",
    "        \n",
    "        for post in posts_data:\n",
    "            user_ids.update(post['likes'])\n",
    "            \n",
    "            for comment in post['comments']:\n",
    "                user_ids.add(comment['from_id'])\n",
    "            \n",
    "            for repost in post['reposts']:\n",
    "                user_ids.add(repost['from_id'])\n",
    "        \n",
    "        return list(user_ids)\n",
    "    \n",
    "    def analyze_two_users(self, user1_input, user2_input, max_posts_per_user=20):\n",
    "        print(f\"Запускаем анализ пользователей {user1_input} и {user2_input}\")\n",
    "        \n",
    "        user1_posts, user1_id = self.parse_user_wall(user1_input, max_posts_per_user)\n",
    "        user2_posts, user2_id = self.parse_user_wall(user2_input, max_posts_per_user)\n",
    "        \n",
    "        all_user_ids = set()\n",
    "        all_user_ids.update(self.collect_all_user_ids(user1_posts))\n",
    "        all_user_ids.update(self.collect_all_user_ids(user2_posts))\n",
    "        \n",
    "        print(f\"Собираем информацию о {len(all_user_ids)} пользователях...\")\n",
    "        user_info = self.get_user_info(list(all_user_ids))\n",
    "        \n",
    "        result = {\n",
    "            'analysis_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'user1_input': user1_input,\n",
    "                'user2_input': user2_input,\n",
    "                'user1_id': user1_id,\n",
    "                'user2_id': user2_id,\n",
    "                'total_posts_analyzed': len(user1_posts) + len(user2_posts),\n",
    "                'total_users_found': len(all_user_ids)\n",
    "            },\n",
    "            'user1_posts': user1_posts,\n",
    "            'user2_posts': user2_posts,\n",
    "            'user_info': user_info\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_to_json(self, data, filename=None):\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user1 = str(data['analysis_info']['user1_input']).replace('/', '_')\n",
    "            user2 = str(data['analysis_info']['user2_input']).replace('/', '_')\n",
    "            filename = f\"vk_analysis_{user1}_{user2}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Данные сохранены в файл: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def generate_interaction_csv(self, user_input, output_filename=None, max_posts=100):\n",
    "        print(f\"Генерируем CSV со статистикой взаимодействий для {user_input}...\")\n",
    "        \n",
    "\n",
    "        posts, owner_id = self.parse_user_wall(user_input, max_posts)\n",
    "        \n",
    "        if not posts:\n",
    "            print(\"Не удалось получить посты пользователя\")\n",
    "            return None\n",
    "        \n",
    "        interactions = defaultdict(lambda: {'likes': 0, 'comments': 0, 'reposts': 0})\n",
    "        \n",
    "        for post in posts:\n",
    "            self._process_post_for_interactions(post, interactions, owner_id)\n",
    "        \n",
    "        if not output_filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user_identifier = str(user_input).replace('/', '_')\n",
    "            output_filename = f\"vk_interactions_{user_identifier}_{timestamp}.csv\"\n",
    "        \n",
    "        return self._write_interactions_to_csv(owner_id, interactions, output_filename)\n",
    "    \n",
    "    def _process_post_for_interactions(self, post, interactions, owner_id):\n",
    "        post_id = post['post_id']\n",
    "        \n",
    "        for like_user_id in post.get('likes', []):\n",
    "            interactions[like_user_id]['likes'] += 1\n",
    "        \n",
    "        for comment in post.get('comments', []):\n",
    "            commenter_id = comment['from_id']\n",
    "            interactions[commenter_id]['comments'] += 1\n",
    "        \n",
    "        for repost in post.get('reposts', []):\n",
    "            reposter_id = repost['from_id']\n",
    "            interactions[reposter_id]['reposts'] += 1\n",
    "    \n",
    "    def _write_interactions_to_csv(self, owner_id, interactions, output_filename):\n",
    "        target_file = 'edges_inference.csv'\n",
    "\n",
    "        try:\n",
    "            edges = pd.read_csv(target_file)\n",
    "        except FileNotFoundError:\n",
    "            edges = pd.DataFrame(columns=[\n",
    "                'target_user_id',\n",
    "                'interactor_id',\n",
    "                'likes_count',\n",
    "                'comments_count',\n",
    "                'reposts_count'\n",
    "            ])\n",
    "\n",
    "\n",
    "        new_rows = []\n",
    "        for interactor_id, stats in interactions.items():\n",
    "            new_rows.append({\n",
    "                'target_user_id': owner_id,\n",
    "                'interactor_id': interactor_id,\n",
    "                'likes_count': stats['likes'],\n",
    "                'comments_count': stats['comments'],\n",
    "                'reposts_count': stats['reposts']\n",
    "            })\n",
    "\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            edges = pd.concat([edges, new_df], ignore_index=True)\n",
    "\n",
    "        print(edges)\n",
    "        edges.to_csv(target_file, index=False)\n",
    "\n",
    "        print(f\"CSV файл обновлён: {target_file}\")\n",
    "        print(f\"Добавлено взаимодействий: {len(new_rows)}\")\n",
    "        return target_file\n",
    "\n",
    "def edges_parser(wall_id):\n",
    "    # Настройки\n",
    "    VK_ACCESS_TOKEN = '6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace'\n",
    "    # Создаем парсер\n",
    "    parser = VKParser(VK_ACCESS_TOKEN)\n",
    "\n",
    "    csv_file = parser.generate_interaction_csv(wall_id, max_posts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6421be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерируем CSV со статистикой взаимодействий для 432213567...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 432213567 ===\n",
      "Получаем посты со стены пользователя 432213567...\n",
      "Получено 2 постов\n",
      "Пост 1/2\n",
      "Обрабатываем пост 84...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Ошибка VK API: Flood control: too many requests from your IP\n",
      "Пост 2/2\n",
      "Обрабатываем пост 124...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Ошибка VK API: Flood control: too many requests from your IP\n",
      "    target_user_id  interactor_id  likes_count  comments_count  reposts_count\n",
      "0        260762184      189326838            1               0              0\n",
      "1        260762184      260762184            1               0              0\n",
      "2        260762184      370224313            1               0              0\n",
      "3        260762184      400282413            1               0              0\n",
      "4        260762184      189326838            1               0              0\n",
      "5        260762184      260762184            1               0              0\n",
      "6        260762184      370224313            1               0              0\n",
      "7        260762184      400282413            1               0              0\n",
      "8        260762184      367322161            1               0              0\n",
      "9        260762184      384336721            1               0              0\n",
      "10       432213567       62129467            1               0              0\n",
      "11       432213567      147025975            1               0              0\n",
      "12       432213567      680254500            1               0              0\n",
      "CSV файл обновлён: edges_inference.csv\n",
      "Добавлено взаимодействий: 3\n",
      "Генерируем CSV со статистикой взаимодействий для 476459668...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 476459668 ===\n",
      "Получаем посты со стены пользователя 476459668...\n",
      "Получено 2 постов\n",
      "Пост 1/2\n",
      "Обрабатываем пост 11...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Ошибка VK API: Flood control: too many requests from your IP\n",
      "Пост 2/2\n",
      "Обрабатываем пост 10...\n",
      "  Собираем лайки...\n",
      "  Собираем комментарии...\n",
      "  Собираем репосты...\n",
      "Ошибка VK API: Flood control: too many requests from your IP\n",
      "    target_user_id  interactor_id  likes_count  comments_count  reposts_count\n",
      "0        260762184      189326838            1               0              0\n",
      "1        260762184      260762184            1               0              0\n",
      "2        260762184      370224313            1               0              0\n",
      "3        260762184      400282413            1               0              0\n",
      "4        260762184      189326838            1               0              0\n",
      "5        260762184      260762184            1               0              0\n",
      "6        260762184      370224313            1               0              0\n",
      "7        260762184      400282413            1               0              0\n",
      "8        260762184      367322161            1               0              0\n",
      "9        260762184      384336721            1               0              0\n",
      "10       432213567       62129467            1               0              0\n",
      "11       432213567      147025975            1               0              0\n",
      "12       432213567      680254500            1               0              0\n",
      "CSV файл обновлён: edges_inference.csv\n",
      "Добавлено взаимодействий: 0\n"
     ]
    }
   ],
   "source": [
    "for i in profile_names:\n",
    "    edges_parser(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "222ea7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_inference.json', 'r', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "    nodes_inference = pd.json_normalize(json_file['people'])\n",
    "\n",
    "edges_inference = pd.read_csv('edges_inference.csv')\n",
    "sampled_edges_inference = edges_inference.drop_duplicates(subset='target_user_id')\n",
    "set_ids2 = list(set(sampled_edges_inference['target_user_id']) | set(sampled_edges_inference['interactor_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed01acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship = pd.DataFrame(json_file['edges']).drop(columns=[2]).rename(columns={0: 'user_id', 1: 'friend_id'})\n",
    "\n",
    "friendship_set = set(friendship.apply(lambda row: frozenset([row['user_id'], row['friend_id']]), axis=1))\n",
    "\n",
    "\n",
    "def is_friend_pair(row):\n",
    "    pair = frozenset([row['target_user_id'], row['interactor_id']])\n",
    "    return pair in friendship_set\n",
    "\n",
    "\n",
    "edges_inference['is_friend'] = edges_inference.apply(is_friend_pair, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eee69f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = set(friendship['user_id']) | set(friendship['friend_id'])\n",
    "\n",
    "edges_inference = edges_inference[\n",
    "    edges_inference['target_user_id'].isin(valid_users) &\n",
    "    edges_inference['interactor_id'].isin(valid_users)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cf5fcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>comments_count</th>\n",
       "      <th>common_friends_count</th>\n",
       "      <th>is_friend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62129467</td>\n",
       "      <td>432213567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147025975</td>\n",
       "      <td>432213567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user1      user2  likes_count  comments_count  common_friends_count  \\\n",
       "0   62129467  432213567            1               0                     0   \n",
       "1  147025975  432213567            1               0                     0   \n",
       "\n",
       "   is_friend  \n",
       "0       True  \n",
       "1       True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "friends_dict = defaultdict(set)\n",
    "for _, row in friendship.iterrows():\n",
    "    friends_dict[row['user_id']].add(row['friend_id'])\n",
    "    friends_dict[row['friend_id']].add(row['user_id'])\n",
    "\n",
    "\n",
    "def count_common_friends(row):\n",
    "    u1 = row['target_user_id']\n",
    "    u2 = row['interactor_id']\n",
    "    return len(friends_dict[u1] & friends_dict[u2])\n",
    "\n",
    "\n",
    "edges_inference['common_friends_count'] = edges_inference.apply(count_common_friends, axis=1)\n",
    "\n",
    "edges_inference['user1'] = edges_inference[['target_user_id', 'interactor_id']].min(axis=1)\n",
    "edges_inference['user2'] = edges_inference[['target_user_id', 'interactor_id']].max(axis=1)\n",
    "aggregated = edges_inference.groupby(['user1', 'user2']).agg({\n",
    "    'likes_count': 'sum',\n",
    "    'comments_count': 'sum',\n",
    "    'common_friends_count': 'first',\n",
    "    'is_friend': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80be7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pickle\n",
    "\n",
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "feature_columns = ['likes_count', 'comments_count', 'common_friends_count']\n",
    "\n",
    "X = aggregated[feature_columns].fillna(0)\n",
    "y = aggregated['is_friend'].astype(int)\n",
    "\n",
    "y_pred = loaded_model.predict(X)\n",
    "y_pred_proba = loaded_model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63f59262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
