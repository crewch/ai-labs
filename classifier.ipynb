{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d7a51d",
   "metadata": {},
   "source": [
    "#### Ввод числовых id двух людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53e720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_names = [242504512, 265329539]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ede27",
   "metadata": {},
   "source": [
    "#### Импорт библиотек и модулей парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51fce122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lab1.parser_vk.lib.vk_friends_parser import VKClient, VKFriendsParser\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854321a2",
   "metadata": {},
   "source": [
    "#### Скачивание информации\n",
    "\n",
    "В этой ячейке в файл json скачивается информация о двух пользователях, их друзьях первого уровня (парсер для второго уровня в частности реализован в main.py).\n",
    "\n",
    "В people расположены признаки людей, а в edges id дружеских связей.\n",
    "\n",
    "Парсер импортирован из lab1.parser_vk.lib.vk_friends_parser\n",
    "\n",
    "Также тут реализовано создание файлов nodes_inference.json и edges_inference.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6177428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы инициализированы:\n",
      " - nodes_inference.json\n",
      " - edges_inference.csv\n",
      "=== Глубина 1/10, обрабатываем 1 пользователей ===\n",
      "=== Глубина 2/10, обрабатываем 166 пользователей ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     47\u001b[39m root_id = profile_name\n\u001b[32m     49\u001b[39m parser = VKFriendsParser(\n\u001b[32m     50\u001b[39m     vk_client=vk, save_photos=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     51\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_network_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroot_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m temp_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muser_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m parser.save_json(temp_filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\parser_vk\\lib\\vk_friends_parser.py:301\u001b[39m, in \u001b[36mVKFriendsParser.fetch_network_fast\u001b[39m\u001b[34m(self, root_ids, depth, include_root_meta)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# --- пакетное получение метаданных друзей ---\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m friend_ids:\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     friend_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_users_bulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfriend_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     found_ids = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m friend_infos:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\parser_vk\\lib\\vk_friends_parser.py:137\u001b[39m, in \u001b[36mVKClient.get_users_bulk\u001b[39m\u001b[34m(self, user_ids, fields)\u001b[39m\n\u001b[32m    135\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33muser_ids\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, chunk)), \u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m: fields}\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musers.get\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    139\u001b[39m         results.extend(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\parser_vk\\lib\\vk_friends_parser.py:99\u001b[39m, in \u001b[36mVKClient._call\u001b[39m\u001b[34m(self, method, params)\u001b[39m\n\u001b[32m     97\u001b[39m url = \u001b[38;5;28mself\u001b[39m.base + method\n\u001b[32m     98\u001b[39m p = {**params, \u001b[33m\"\u001b[39m\u001b[33maccess_token\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.token, \u001b[33m\"\u001b[39m\u001b[33mv\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.v}\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m resp.raise_for_status()\n\u001b[32m    101\u001b[39m data = resp.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1378\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1380\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# edges = pd.read_csv('edges.csv')\n",
    "# sampled_edges = edges.drop_duplicates(subset='target_user_id')\n",
    "# sampled_edges = sampled_edges.sample(n=100)\n",
    "# set_ids = list(set(sampled_edges['target_user_id']) | set(sampled_edges['interactor_id']))\n",
    "\n",
    "\n",
    "# def nodes_maker(profile_names):\n",
    "\n",
    "\n",
    "def init_inference_files():\n",
    "    nodes_path = \"nodes_inference.json\"\n",
    "    with open(nodes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    edges_path = \"edges_inference.csv\"\n",
    "    with open(edges_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"target_user_id\", \"interactor_id\", \"likes_count\", \"comments_count\", \"reposts_count\"])\n",
    "\n",
    "    print(\"Файлы инициализированы:\")\n",
    "    print(f\" - {nodes_path}\")\n",
    "    print(f\" - {edges_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_inference_files()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VK_SERVICE_ACCESS_TOKEN = \"6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace\"\n",
    "# VK_SERVICE_ACCESS_TOKEN = os.getenv(\"VK_SERVICE_ACCESS_TOKEN\", None)\n",
    "# if VK_SERVICE_ACCESS_TOKEN is None:\n",
    "#     raise ValueError(\"VK_SERVICE_ACCESS_TOKEN не найден в переменных окружения\")\n",
    "\n",
    "vk = VKClient(token=VK_SERVICE_ACCESS_TOKEN)\n",
    "\n",
    "\n",
    "if os.path.exists(\"nodes_inference.json\"):\n",
    "    with open(\"nodes_inference.json\", 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            all_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            all_data = []\n",
    "else:\n",
    "    all_data = []\n",
    "\n",
    "for profile_name in profile_names:\n",
    "    root_id = profile_name\n",
    "\n",
    "    parser = VKFriendsParser(\n",
    "        vk_client=vk, save_photos=False\n",
    "    )\n",
    "    parser.fetch_network_fast([root_id], depth=1)\n",
    "\n",
    "    temp_filename = f\"user_{root_id}.json\"\n",
    "    parser.save_json(temp_filename)\n",
    "    print(f\"Данные сохранены в {temp_filename}\")\n",
    "    \n",
    "    with open(temp_filename, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            user_data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Ошибка чтения {temp_filename}: {e}\")\n",
    "            continue\n",
    "    print('++++++++++++++++')\n",
    "\n",
    "\n",
    "    if (user_data['people']):\n",
    "        # random_friend = random.choice(user_data['people'])\n",
    "        # random_friend_id = random.choice(user_data['people'])['id']\n",
    "        # print(random_friend)\n",
    "        # print(random_friend_id)\n",
    "        # parser.fetch_network_fast([random_friend_id], depth=1)\n",
    "\n",
    "        # # Сохраняем во временный файл\n",
    "        # temp_filename2 = f\"user_{random_friend_id}.json\"\n",
    "        # parser.save_json(temp_filename2)\n",
    "        # print(f\"Данные сохранены в {temp_filename2}\")\n",
    "        \n",
    "        # Читаем данные из временного файла\n",
    "        # with open(temp_filename2, 'r', encoding='utf-8') as f:\n",
    "        #     try:\n",
    "        #         user_data2 = json.load(f)\n",
    "        #     except json.JSONDecodeError as e:\n",
    "        #         print(f\"Ошибка чтения {temp_filename2}: {e}\")\n",
    "        #         continue\n",
    "\n",
    "\n",
    "        if not isinstance(all_data, dict):\n",
    "            all_data = {\"people\": [], \"edges\": []}\n",
    "\n",
    "        if isinstance(user_data, dict) and \"people\" in user_data and \"edges\" in user_data:\n",
    "            all_data[\"people\"].extend(user_data[\"people\"])\n",
    "            all_data[\"edges\"].extend(user_data[\"edges\"])\n",
    "            # all_data[\"people\"].extend(user_data2[\"people\"])\n",
    "            # all_data[\"edges\"].extend(user_data2[\"edges\"])\n",
    "        else:\n",
    "            print(f\"Предупреждение: Некорректная структура данных для {profile_name}\")\n",
    "        \n",
    "        if os.path.exists(temp_filename):\n",
    "            os.remove(temp_filename)\n",
    "        # if os.path.exists(temp_filename2):\n",
    "        #     os.remove(temp_filename2)\n",
    "        print(f\"Данные пользователя {profile_name} добавлены в общий список\")\n",
    "\n",
    "with open(\"nodes_inference.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=5)\n",
    "\n",
    "print(\"Все данные объединены в nodes_inference.json\")\n",
    "\n",
    "\n",
    "# with open('nodes.json', 'r', encoding='utf-8') as f:\n",
    "#     json_file = json.load(f)\n",
    "\n",
    "# nodes_ids = pd.json_normalize(json_file['people'])['id'].unique()\n",
    "\n",
    "# for i in range(len(nodes_ids)):\n",
    "#     print(i)\n",
    "#     print(i/len(nodes_ids))\n",
    "#     node_id = nodes_ids[i]\n",
    "#     edges_parser(node_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30691f2",
   "metadata": {},
   "source": [
    "#### Скачивание в csv количества комментариев, лайков и репостов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ac629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VKParser:\n",
    "    def __init__(self, access_token, version='5.131'):\n",
    "        self.access_token = access_token\n",
    "        self.version = version\n",
    "        self.base_url = 'https://api.vk.com/method/'\n",
    "        \n",
    "    def make_request(self, method, params):\n",
    "        \"\"\"Базовый метод для запросов к VK API\"\"\"\n",
    "        url = f\"{self.base_url}{method}\"\n",
    "        params.update({\n",
    "            'access_token': self.access_token,\n",
    "            'v': self.version\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Ошибка VK API: {data['error']['error_msg']}\")\n",
    "                return None\n",
    "                \n",
    "            return data['response']\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка запроса: {e}\")\n",
    "            return None\n",
    "\n",
    "    def resolve_screen_name(self, screen_name):\n",
    "        print(f\"Преобразуем короткое имя '{screen_name}' в ID...\")\n",
    "        \n",
    "        response = self.make_request('utils.resolveScreenName', {\n",
    "            'screen_name': screen_name\n",
    "        })\n",
    "        \n",
    "        if response and 'object_id' in response:\n",
    "            user_id = response['object_id']\n",
    "            print(f\"Найден ID: {user_id}\")\n",
    "            return user_id\n",
    "        else:\n",
    "            print(f\"Не удалось найти ID для '{screen_name}'\")\n",
    "            return None\n",
    "\n",
    "    def get_user_id(self, user_input):\n",
    "\n",
    "        if isinstance(user_input, int) or (isinstance(user_input, str) and user_input.isdigit()):\n",
    "            return int(user_input)\n",
    "        \n",
    "        # Если это короткое имя, преобразуем в ID\n",
    "        if isinstance(user_input, str) and not user_input.startswith('-'):\n",
    "            return self.resolve_screen_name(user_input)\n",
    "        \n",
    "        return user_input\n",
    "\n",
    "    def get_wall_posts(self, owner_id, count=100):\n",
    "        \"\"\"Получает посты со стены пользователя\"\"\"\n",
    "        print(f\"Получаем посты со стены пользователя {owner_id}...\")\n",
    "        \n",
    "        # Преобразуем owner_id в числовой формат если нужно\n",
    "        numeric_owner_id = self.get_user_id(owner_id)\n",
    "        if numeric_owner_id is None:\n",
    "            print(f\"Не удалось определить ID для {owner_id}\")\n",
    "            return []\n",
    "        \n",
    "        posts = []\n",
    "        offset = 0\n",
    "        max_posts = count\n",
    "        \n",
    "        while len(posts) < max_posts:\n",
    "            response = self.make_request('wall.get', {\n",
    "                'owner_id': numeric_owner_id,\n",
    "                'count': min(100, max_posts - len(posts)),\n",
    "                'offset': offset,\n",
    "                'extended': 1\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            posts.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) == 0:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        print(f\"Получено {len(posts)} постов\")\n",
    "\n",
    "        posts_count = pd.read_csv('posts_count.csv')\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'id': [numeric_owner_id],\n",
    "            'posts': [len(posts)]\n",
    "        })\n",
    "\n",
    "        posts_count = pd.concat([posts_count, new_row], ignore_index=True)\n",
    "        posts_count.to_csv('posts_count.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "        return posts, numeric_owner_id\n",
    "    \n",
    "    def get_likes(self, owner_id, item_id, item_type='post'):\n",
    "        \"\"\"Получает список пользователей, лайкнувших запись\"\"\"\n",
    "        likes = []\n",
    "        offset = 0\n",
    "        count = 1000\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('likes.getList', {\n",
    "                'type': item_type,\n",
    "                'owner_id': owner_id,\n",
    "                'item_id': item_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'filter': 'likes'\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            likes.extend(response['items'])\n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return likes\n",
    "    \n",
    "    def get_comments(self, owner_id, post_id):\n",
    "        \"\"\"Получает комментарии к посту\"\"\"\n",
    "        comments = []\n",
    "        offset = 0\n",
    "        count = 100\n",
    "        \n",
    "        while True:\n",
    "            response = self.make_request('wall.getComments', {\n",
    "                'owner_id': owner_id,\n",
    "                'post_id': post_id,\n",
    "                'count': count,\n",
    "                'offset': offset,\n",
    "                'extended': 0\n",
    "            })\n",
    "            \n",
    "            if not response or 'items' not in response:\n",
    "                break\n",
    "                \n",
    "            for comment in response['items']:\n",
    "                comment_data = {\n",
    "                    'id': comment['id'],\n",
    "                    'from_id': comment['from_id'],\n",
    "                    'date': comment['date'],\n",
    "                    'text': comment['text'],\n",
    "                    'likes': comment.get('likes', {}).get('count', 0)\n",
    "                }\n",
    "                comments.append(comment_data)\n",
    "            \n",
    "            offset += len(response['items'])\n",
    "            \n",
    "            if len(response['items']) < count:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.34)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def get_reposts(self, owner_id, post_id):\n",
    "        \"\"\"Получает информацию о репостах\"\"\"\n",
    "        response = self.make_request('wall.getReposts', {\n",
    "            'owner_id': owner_id,\n",
    "            'post_id': post_id,\n",
    "            'count': 1000\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return []\n",
    "            \n",
    "        reposts = []\n",
    "        if 'items' in response:\n",
    "            for repost in response['items']:\n",
    "                repost_data = {\n",
    "                    'id': repost['id'],\n",
    "                    'from_id': repost['from_id'],\n",
    "                    'date': repost['date'],\n",
    "                    'text': repost.get('text', ''),\n",
    "                    'copy_history': repost.get('copy_history', [])\n",
    "                }\n",
    "                reposts.append(repost_data)\n",
    "        \n",
    "        return reposts\n",
    "    \n",
    "    def get_user_info(self, user_ids):\n",
    "        \"\"\"Получает информацию о пользователях\"\"\"\n",
    "        if not user_ids:\n",
    "            return {}\n",
    "            \n",
    "        numeric_ids = []\n",
    "        for user_id in user_ids:\n",
    "            if isinstance(user_id, int) or (isinstance(user_id, str) and user_id.lstrip('-').isdigit()):\n",
    "                numeric_ids.append(str(user_id))\n",
    "        \n",
    "        if not numeric_ids:\n",
    "            return {}\n",
    "            \n",
    "        response = self.make_request('users.get', {\n",
    "            'user_ids': ','.join(numeric_ids),\n",
    "            'fields': 'first_name,last_name,sex,bdate,city,country,screen_name'\n",
    "        })\n",
    "        \n",
    "        if not response:\n",
    "            return {}\n",
    "            \n",
    "        user_info = {}\n",
    "        for user in response:\n",
    "            user_info[user['id']] = {\n",
    "                'first_name': user.get('first_name', ''),\n",
    "                'last_name': user.get('last_name', ''),\n",
    "                'sex': user.get('sex', 0),\n",
    "                'bdate': user.get('bdate', ''),\n",
    "                'city': user.get('city', {}).get('title', '') if 'city' in user else '',\n",
    "                'country': user.get('country', {}).get('title', '') if 'country' in user else '',\n",
    "                'screen_name': user.get('screen_name', '')\n",
    "            }\n",
    "        \n",
    "        return user_info\n",
    "    \n",
    "    def process_post(self, post, owner_id):\n",
    "        \"\"\"Обрабатывает один пост и собирает всю информацию\"\"\"\n",
    "        post_id = post['id']\n",
    "        \n",
    "        print(f\"Обрабатываем пост {post_id}...\")\n",
    "        \n",
    "        post_data = {\n",
    "            'post_id': post_id,\n",
    "            'owner_id': owner_id,\n",
    "            'date': post['date'],\n",
    "            'text': post.get('text', '')[:500],\n",
    "            'likes_count': post.get('likes', {}).get('count', 0),\n",
    "            'comments_count': post.get('comments', {}).get('count', 0),\n",
    "            'reposts_count': post.get('reposts', {}).get('count', 0),\n",
    "            'views_count': post.get('views', {}).get('count', 0) if 'views' in post else 0\n",
    "        }\n",
    "        \n",
    "        if 'copy_history' in post and post['copy_history']:\n",
    "            original_post = post['copy_history'][0]\n",
    "            post_data['is_repost'] = True\n",
    "            post_data['reposted_from'] = {\n",
    "                'owner_id': original_post['owner_id'],\n",
    "                'post_id': original_post['id'],\n",
    "                'text': original_post.get('text', '')[:500]\n",
    "            }\n",
    "        else:\n",
    "            post_data['is_repost'] = False\n",
    "            post_data['reposted_from'] = None\n",
    "        \n",
    "        if isinstance(owner_id, int):\n",
    "            print(f\"  Собираем лайки...\")\n",
    "            post_data['likes'] = self.get_likes(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем комментарии...\")\n",
    "            post_data['comments'] = self.get_comments(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "            \n",
    "            print(f\"  Собираем репосты...\")\n",
    "            post_data['reposts'] = self.get_reposts(owner_id, post_id)\n",
    "            time.sleep(0.34)\n",
    "        else:\n",
    "            print(f\"  Пропускаем сбор лайков/комментариев - неверный owner_id\")\n",
    "            post_data['likes'] = []\n",
    "            post_data['comments'] = []\n",
    "            post_data['reposts'] = []\n",
    "        \n",
    "        return post_data\n",
    "    \n",
    "    def parse_user_wall(self, user_input, max_posts=50):\n",
    "        print(f\"\\n=== Начинаем парсинг стены пользователя {user_input} ===\")\n",
    "        \n",
    "        posts, numeric_owner_id = self.get_wall_posts(user_input, max_posts)\n",
    "        processed_posts = []\n",
    "        \n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"Пост {i+1}/{len(posts)}\")\n",
    "            processed_post = self.process_post(post, numeric_owner_id)\n",
    "            processed_posts.append(processed_post)\n",
    "            \n",
    "            if i < len(posts) - 1:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return processed_posts, numeric_owner_id\n",
    "    \n",
    "    def collect_all_user_ids(self, posts_data):\n",
    "        user_ids = set()\n",
    "        \n",
    "        for post in posts_data:\n",
    "            user_ids.update(post['likes'])\n",
    "            \n",
    "            for comment in post['comments']:\n",
    "                user_ids.add(comment['from_id'])\n",
    "            \n",
    "            for repost in post['reposts']:\n",
    "                user_ids.add(repost['from_id'])\n",
    "        \n",
    "        return list(user_ids)\n",
    "    \n",
    "    def analyze_two_users(self, user1_input, user2_input, max_posts_per_user=20):\n",
    "        print(f\"Запускаем анализ пользователей {user1_input} и {user2_input}\")\n",
    "        \n",
    "        user1_posts, user1_id = self.parse_user_wall(user1_input, max_posts_per_user)\n",
    "        user2_posts, user2_id = self.parse_user_wall(user2_input, max_posts_per_user)\n",
    "        \n",
    "        all_user_ids = set()\n",
    "        all_user_ids.update(self.collect_all_user_ids(user1_posts))\n",
    "        all_user_ids.update(self.collect_all_user_ids(user2_posts))\n",
    "        \n",
    "        print(f\"Собираем информацию о {len(all_user_ids)} пользователях...\")\n",
    "        user_info = self.get_user_info(list(all_user_ids))\n",
    "        \n",
    "        result = {\n",
    "            'analysis_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'user1_input': user1_input,\n",
    "                'user2_input': user2_input,\n",
    "                'user1_id': user1_id,\n",
    "                'user2_id': user2_id,\n",
    "                'total_posts_analyzed': len(user1_posts) + len(user2_posts),\n",
    "                'total_users_found': len(all_user_ids)\n",
    "            },\n",
    "            'user1_posts': user1_posts,\n",
    "            'user2_posts': user2_posts,\n",
    "            'user_info': user_info\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_to_json(self, data, filename=None):\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user1 = str(data['analysis_info']['user1_input']).replace('/', '_')\n",
    "            user2 = str(data['analysis_info']['user2_input']).replace('/', '_')\n",
    "            filename = f\"vk_analysis_{user1}_{user2}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Данные сохранены в файл: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def generate_interaction_csv(self, user_input, output_filename=None, max_posts=100):\n",
    "        print(f\"Генерируем CSV со статистикой взаимодействий для {user_input}...\")\n",
    "        \n",
    "\n",
    "        posts, owner_id = self.parse_user_wall(user_input, max_posts)\n",
    "        \n",
    "        if not posts:\n",
    "            print(\"Не удалось получить посты пользователя\")\n",
    "            return None\n",
    "        \n",
    "        interactions = defaultdict(lambda: {'likes': 0, 'comments': 0, 'reposts': 0})\n",
    "        \n",
    "        for post in posts:\n",
    "            self._process_post_for_interactions(post, interactions, owner_id)\n",
    "        \n",
    "        if not output_filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            user_identifier = str(user_input).replace('/', '_')\n",
    "            output_filename = f\"vk_interactions_{user_identifier}_{timestamp}.csv\"\n",
    "        \n",
    "        return self._write_interactions_to_csv(owner_id, interactions, output_filename)\n",
    "    \n",
    "    def _process_post_for_interactions(self, post, interactions, owner_id):\n",
    "        post_id = post['post_id']\n",
    "        \n",
    "        for like_user_id in post.get('likes', []):\n",
    "            interactions[like_user_id]['likes'] += 1\n",
    "        \n",
    "        for comment in post.get('comments', []):\n",
    "            commenter_id = comment['from_id']\n",
    "            interactions[commenter_id]['comments'] += 1\n",
    "        \n",
    "        for repost in post.get('reposts', []):\n",
    "            reposter_id = repost['from_id']\n",
    "            interactions[reposter_id]['reposts'] += 1\n",
    "    \n",
    "    def _write_interactions_to_csv(self, owner_id, interactions, output_filename):\n",
    "        target_file = 'edges_inference.csv'\n",
    "\n",
    "        try:\n",
    "            edges = pd.read_csv(target_file)\n",
    "        except FileNotFoundError:\n",
    "            edges = pd.DataFrame(columns=[\n",
    "                'target_user_id',\n",
    "                'interactor_id',\n",
    "                'likes_count',\n",
    "                'comments_count',\n",
    "                'reposts_count'\n",
    "            ])\n",
    "\n",
    "\n",
    "        new_rows = []\n",
    "        for interactor_id, stats in interactions.items():\n",
    "            new_rows.append({\n",
    "                'target_user_id': owner_id,\n",
    "                'interactor_id': interactor_id,\n",
    "                'likes_count': stats['likes'],\n",
    "                'comments_count': stats['comments'],\n",
    "                'reposts_count': stats['reposts']\n",
    "            })\n",
    "\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            edges = pd.concat([edges, new_df], ignore_index=True)\n",
    "\n",
    "        print(edges)\n",
    "        edges.to_csv(target_file, index=False)\n",
    "\n",
    "        print(f\"CSV файл обновлён: {target_file}\")\n",
    "        print(f\"Добавлено взаимодействий: {len(new_rows)}\")\n",
    "        return target_file\n",
    "\n",
    "def edges_parser(wall_id):\n",
    "    # Настройки\n",
    "    VK_ACCESS_TOKEN = '6fdea0906fdea0906fdea0900e6ce5f2d866fde6fdea0900738eb840c756af0caf0aace'\n",
    "    # Создаем парсер\n",
    "    parser = VKParser(VK_ACCESS_TOKEN)\n",
    "\n",
    "    csv_file = parser.generate_interaction_csv(wall_id, max_posts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерируем CSV со статистикой взаимодействий для 242504512...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 242504512 ===\n",
      "Получаем посты со стены пользователя 242504512...\n",
      "Получено 0 постов\n",
      "Не удалось получить посты пользователя\n",
      "Генерируем CSV со статистикой взаимодействий для 265329539...\n",
      "\n",
      "=== Начинаем парсинг стены пользователя 265329539 ===\n",
      "Получаем посты со стены пользователя 265329539...\n",
      "Получено 0 постов\n",
      "Не удалось получить посты пользователя\n"
     ]
    }
   ],
   "source": [
    "for i in profile_names:\n",
    "    edges_parser(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a3c89",
   "metadata": {},
   "source": [
    "#### Загрузка json и csv файлов в дата фреймы признаков вершин и рёбер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ea7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_inference.json', 'r', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "    nodes_inference = pd.json_normalize(json_file['people'])\n",
    "\n",
    "edges_inference = pd.read_csv('edges_inference.csv')\n",
    "# sampled_edges_inference = edges_inference.drop_duplicates(subset='target_user_id')\n",
    "# set_ids2 = list(set(sampled_edges_inference['target_user_id']) | set(sampled_edges_inference['interactor_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab082a",
   "metadata": {},
   "source": [
    "#### Определение наличия дружбы между людьми и добавление в дата фрейм рёбер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship = pd.DataFrame(json_file['edges']).drop(columns=[2]).rename(columns={0: 'user_id', 1: 'friend_id'})\n",
    "\n",
    "friendship_set = set(friendship.apply(lambda row: frozenset([row['user_id'], row['friend_id']]), axis=1))\n",
    "\n",
    "\n",
    "def is_friend_pair(row):\n",
    "    pair = frozenset([row['target_user_id'], row['interactor_id']])\n",
    "    return pair in friendship_set\n",
    "\n",
    "\n",
    "edges_inference['is_friend'] = edges_inference.apply(is_friend_pair, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19b661",
   "metadata": {},
   "source": [
    "#### Агрегация информации у целевых пользователей и обработка в случае отсутствия связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target_user_id  interactor_id  likes_count  comments_count  reposts_count  \\\n",
      "0       242504512      265329539            0               0              0   \n",
      "\n",
      "   common_friends_count  is_friend  \n",
      "0                     0          1  \n"
     ]
    }
   ],
   "source": [
    "friends_dict = defaultdict(set)\n",
    "for _, row in friendship.iterrows():\n",
    "    friends_dict[row['user_id']].add(row['friend_id'])\n",
    "    friends_dict[row['friend_id']].add(row['user_id'])\n",
    "\n",
    "\n",
    "def count_common_friends(u1, u2):\n",
    "    return len(friends_dict[u1] & friends_dict[u2])\n",
    "\n",
    "\n",
    "def check_is_friend(u1, u2):\n",
    "    return int(u2 in friends_dict[u1])\n",
    "\n",
    "\n",
    "edges_inference['common_friends_count'] = edges_inference.apply(\n",
    "    lambda row: count_common_friends(row['target_user_id'], row['interactor_id']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "mask = (\n",
    "    ((edges_inference['target_user_id'] == profile_names[0]) & (edges_inference['interactor_id'] == profile_names[1])) |\n",
    "    ((edges_inference['target_user_id'] == profile_names[1]) & (edges_inference['interactor_id'] == profile_names[0]))\n",
    ")\n",
    "\n",
    "pair_edges = edges_inference[mask]\n",
    "\n",
    "\n",
    "common_friends = count_common_friends(profile_names[0], profile_names[1])\n",
    "is_friend = check_is_friend(profile_names[0], profile_names[1])\n",
    "\n",
    "\n",
    "if pair_edges.empty:\n",
    "    agg_row = {\n",
    "        'target_user_id': profile_names[0],\n",
    "        'interactor_id': profile_names[1],\n",
    "        'likes_count': 0,\n",
    "        'comments_count': 0,\n",
    "        'reposts_count': 0,\n",
    "        'common_friends_count': common_friends,\n",
    "        'is_friend': is_friend\n",
    "    }\n",
    "else:\n",
    "    agg_row = {\n",
    "        'target_user_id': profile_names[0],\n",
    "        'interactor_id': profile_names[1],\n",
    "        'likes_count': pair_edges['likes_count'].sum(),\n",
    "        'comments_count': pair_edges['comments_count'].sum(),\n",
    "        'reposts_count': pair_edges['reposts_count'].sum(),\n",
    "        'common_friends_count': common_friends,\n",
    "        'is_friend': is_friend\n",
    "    }\n",
    "\n",
    "result_df = pd.DataFrame([agg_row])\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce51ce6",
   "metadata": {},
   "source": [
    "#### Загрузка предобученной модели и получение предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "feature_columns = ['likes_count', 'comments_count', 'common_friends_count']\n",
    "\n",
    "\n",
    "\n",
    "X = result_df[feature_columns].fillna(0)\n",
    "y = result_df['is_friend'].astype(int)\n",
    "\n",
    "y_pred = loaded_model.predict(X)\n",
    "y_pred_proba = loaded_model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f59262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "[[1]]\n",
      "[0.76269142]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vsu70\\Desktop\\ai-labs-1\\lab1\\ai2_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred))\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print(y_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
